{"cells": [{"cell_type": "code", "execution_count": 2, "id": "3b1bc70f-9fa3-4d46-83bd-d22efdca1a27", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/02/05 06:27:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "\n# Import SparkSession\nfrom pyspark.sql import SparkSession\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"Handling Dates in PySpark\") \\\n    .getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "id": "be28abef-109f-4895-96ce-8b19a4430a17", "metadata": {}, "outputs": [], "source": "# Sending the Data To HDFS Via Notebook\n\n# Create a CSV file\ncsv_data = \"\"\"id,date_iso,date_dmy,date_mdy,timestamp\n1,2023-01-15,15/01/2023,01/15/2023,2023-01-15 10:30:00\n2,2023-05-20,20/05/2023,05/20/2023,2023-05-20 15:45:00\n3,InvalidDate,31/02/2023,02/31/2023,InvalidTimestamp\n4,,,,\n\"\"\"\n\n# Save the CSV file\nwith open(\"dates_data.csv\", \"w\") as f:\n    f.write(csv_data)\n"}, {"cell_type": "code", "execution_count": 5, "id": "9ed8440e-b7d0-4f02-b3af-c89213cc7295", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "dates_data.csv\n"}], "source": "!ls *dates_data*"}, {"cell_type": "code", "execution_count": 6, "id": "89163d62-f5c5-4e87-aba8-10fa0961e5ad", "metadata": {}, "outputs": [], "source": "!hadoop fs -put dates_data.csv /data/dates_data.csv"}, {"cell_type": "code", "execution_count": 7, "id": "98c6d79e-ef98-48ec-a41d-dec360c2d8c9", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 12 items\n-rw-r--r--   2 root hadoop  343317147 2025-02-03 11:53 /data/customers.csv\n-rw-r--r--   2 root hadoop       5488 2025-02-03 11:53 /data/customers_100.csv\n-rw-r--r--   2 root hadoop   10528211 2025-02-03 11:53 /data/customers_10mb.csv\n-rw-r--r--   2 root hadoop    1060750 2025-02-03 11:53 /data/customers_1mb.csv\n-rw-r--r--   2 root hadoop  570783961 2025-02-03 11:53 /data/customers_500mb.csv\n-rw-r--r--   2 root hadoop        351 2025-02-04 05:35 /data/customers_with_errors.csv\n-rw-r--r--   2 root hadoop         23 2025-02-04 14:53 /data/date.csv\n-rw-r--r--   2 root hadoop        209 2025-02-05 06:28 /data/dates_data.csv\ndrwxr-xr-x   - root hadoop          0 2025-02-04 07:53 /data/write_output.csv\ndrwxr-xr-x   - root hadoop          0 2025-02-04 07:56 /data/write_output_1_part\ndrwxr-xr-x   - root hadoop          0 2025-02-04 07:59 /data/write_output_1_part_2\ndrwxr-xr-x   - root hadoop          0 2025-02-04 08:02 /data/write_output_1_part_3\n"}], "source": "!hadoop fs -ls /data/"}, {"cell_type": "code", "execution_count": null, "id": "1c45e4dd-89da-43fe-82b5-4b7c83237a4b", "metadata": {}, "outputs": [], "source": "# from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\n# # StructType for the schema\n# struct_schema = StructType([\n#     StructField(\"id\", IntegerType(), nullable=True),\n#     StructField(\"date_iso\", StringType(), nullable=True),\n#     StructField(\"date_dmy\", StringType(), nullable=True),\n#     StructField(\"date_mdy\", StringType(), nullable=True),\n#     StructField(\"timestamp\", StringType(), nullable=True)\n# ])\n"}, {"cell_type": "code", "execution_count": 14, "id": "ed4f154b-6a19-496f-aeae-ca987b27a1a8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+----------+--------+--------+-------------------+\n|id |date_iso  |date_dmy|date_mdy|timestamp          |\n+---+----------+--------+--------+-------------------+\n|1  |2023-01-15|null    |null    |2023-01-15 10:30:00|\n|2  |2023-05-20|null    |null    |2023-05-20 15:45:00|\n|3  |null      |null    |null    |null               |\n|4  |null      |null    |null    |null               |\n+---+----------+--------+--------+-------------------+\n\n"}], "source": "\n\n# DDL String for the schema\nddl_schema = \"\"\"\n    id INT,\n    date_iso DATE,\n    date_dmy DATE,\n    date_mdy DATE,\n    timestamp TIMESTAMP\n\"\"\"\n\n\n# Read the CSV file into a DataFrame\ndf_file = spark.read.option(\"header\", True).schema(ddl_schema).csv(\"/data/dates_data.csv\")\n\n\n\n# Show the DataFrame\ndf_file.show(truncate=False)"}, {"cell_type": "code", "execution_count": 16, "id": "f6b582b9-8f02-43ff-9d22-e745fd21fde9", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- id: integer (nullable = true)\n |-- date_iso: date (nullable = true)\n |-- date_dmy: date (nullable = true)\n |-- date_mdy: date (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n\n"}], "source": "df_file.printSchema()"}, {"cell_type": "code", "execution_count": null, "id": "b0847979-f2ee-49a6-843a-de87b747b0ac", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "bc58c7d3-c509-44ca-8d00-5d6c22b7b15c", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "386a79a8-dbf3-458e-924c-0122e3115b43", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "fdc2c750-b0cb-4e5d-bdba-c0f084456ffb", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 8, "id": "adb4a37c-c18c-4160-945e-b0d07a769f5d", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+---+-----------+----------+----------+-------------------+\n|id |date_iso   |date_dmy  |date_mdy  |timestamp          |\n+---+-----------+----------+----------+-------------------+\n|1  |2023-01-15 |15/01/2023|01/15/2023|2023-01-15 10:30:00|\n|2  |2023-05-20 |20/05/2023|05/20/2023|2023-05-20 15:45:00|\n|3  |InvalidDate|31/02/2023|02/31/2023|InvalidTimestamp   |\n|4  |null       |null      |null      |null               |\n+---+-----------+----------+----------+-------------------+\n\n"}], "source": "# Sample data with multiple date formats\ndata = [\n    (1, \"2023-01-15\", \"15/01/2023\", \"01/15/2023\", \"2023-01-15 10:30:00\"),\n    (2, \"2023-05-20\", \"20/05/2023\", \"05/20/2023\", \"2023-05-20 15:45:00\"),\n    (3, \"InvalidDate\", \"31/02/2023\", \"02/31/2023\", \"InvalidTimestamp\"),  # Invalid dates\n    (4, None, None, None, None)  # Null values\n]\n\n# Define column names\ncolumns = [\"id\", \"date_iso\", \"date_dmy\", \"date_mdy\", \"timestamp\"]\n\n\n# Create DataFrame\ndf = spark.createDataFrame(data, schema=columns)\n\n# Show the DataFrame\ndf.show(truncate=False)"}, {"cell_type": "code", "execution_count": 9, "id": "d752126d-c7c0-4f0b-aa01-092105e5c97c", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- id: long (nullable = true)\n |-- date_iso: string (nullable = true)\n |-- date_dmy: string (nullable = true)\n |-- date_mdy: string (nullable = true)\n |-- timestamp: string (nullable = true)\n\n"}], "source": "df.printSchema()"}, {"cell_type": "code", "execution_count": 19, "id": "90cc0758-aaa1-41c5-aa50-234219f89bf9", "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import to_date\n\ndf = df\\\n    .withColumn('parsed_date_iso', to_date(df.date_iso, 'yyyy-MM-dd'))\\\n    .withColumn('parsed_date_dmy', to_date(df.date_dmy, 'dd/MM/yyyy'))\\\n    .withColumn('parsed_date_mdy', to_date(df.date_mdy, 'MM/dd/yyyy'))\n\n"}, {"cell_type": "code", "execution_count": 20, "id": "75843117-3681-4a84-a44b-53ccecb10659", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-----------+----------+----------+-------------------+---------------+---------------+---------------+\n|id |date_iso   |date_dmy  |date_mdy  |timestamp          |parsed_date_iso|parsed_date_dmy|parsed_date_mdy|\n+---+-----------+----------+----------+-------------------+---------------+---------------+---------------+\n|1  |2023-01-15 |15/01/2023|01/15/2023|2023-01-15 10:30:00|2023-01-15     |2023-01-15     |2023-01-15     |\n|2  |2023-05-20 |20/05/2023|05/20/2023|2023-05-20 15:45:00|2023-05-20     |2023-05-20     |2023-05-20     |\n|3  |InvalidDate|31/02/2023|02/31/2023|InvalidTimestamp   |null           |null           |null           |\n|4  |null       |null      |null      |null               |null           |null           |null           |\n+---+-----------+----------+----------+-------------------+---------------+---------------+---------------+\n\n"}], "source": "df.show(truncate=False)"}, {"cell_type": "code", "execution_count": null, "id": "ee049b8b-1805-47e0-b303-cfc139abe195", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "59ba8468-7430-4179-a6aa-cc5d4e7f4707", "metadata": {}, "source": "# Timestamp"}, {"cell_type": "code", "execution_count": 22, "id": "1a47be1e-762b-431c-ad09-d8a37ee6ff50", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-----------+----------+----------+-------------------+---------------+---------------+---------------+-------------------+\n| id|   date_iso|  date_dmy|  date_mdy|          timestamp|parsed_date_iso|parsed_date_dmy|parsed_date_mdy|   parsed_timestamp|\n+---+-----------+----------+----------+-------------------+---------------+---------------+---------------+-------------------+\n|  1| 2023-01-15|15/01/2023|01/15/2023|2023-01-15 10:30:00|     2023-01-15|     2023-01-15|     2023-01-15|2023-01-15 10:30:00|\n|  2| 2023-05-20|20/05/2023|05/20/2023|2023-05-20 15:45:00|     2023-05-20|     2023-05-20|     2023-05-20|2023-05-20 15:45:00|\n|  3|InvalidDate|31/02/2023|02/31/2023|   InvalidTimestamp|           null|           null|           null|               null|\n|  4|       null|      null|      null|               null|           null|           null|           null|               null|\n+---+-----------+----------+----------+-------------------+---------------+---------------+---------------+-------------------+\n\nroot\n |-- id: long (nullable = true)\n |-- date_iso: string (nullable = true)\n |-- date_dmy: string (nullable = true)\n |-- date_mdy: string (nullable = true)\n |-- timestamp: string (nullable = true)\n |-- parsed_date_iso: date (nullable = true)\n |-- parsed_date_dmy: date (nullable = true)\n |-- parsed_date_mdy: date (nullable = true)\n |-- parsed_timestamp: timestamp (nullable = true)\n\n"}], "source": "from pyspark.sql.functions import to_timestamp,year,month,dayofmonth,hour,minute\n\n\ndf = df.withColumn('parsed_timestamp',to_timestamp(df.timestamp, 'yyyy-MM-dd HH:mm:ss'))\ndf.show()\ndf.printSchema()"}, {"cell_type": "code", "execution_count": 25, "id": "edac8f0a-5ec7-4d0c-a1ef-f676afc7a271", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-----------+----------+----------+-------------------+---------------+---------------+---------------+-------------------+----+-----+----+----+------+\n| id|   date_iso|  date_dmy|  date_mdy|          timestamp|parsed_date_iso|parsed_date_dmy|parsed_date_mdy|   parsed_timestamp|year|month| day|hour|minute|\n+---+-----------+----------+----------+-------------------+---------------+---------------+---------------+-------------------+----+-----+----+----+------+\n|  1| 2023-01-15|15/01/2023|01/15/2023|2023-01-15 10:30:00|     2023-01-15|     2023-01-15|     2023-01-15|2023-01-15 10:30:00|2023|    1|  15|  10|    30|\n|  2| 2023-05-20|20/05/2023|05/20/2023|2023-05-20 15:45:00|     2023-05-20|     2023-05-20|     2023-05-20|2023-05-20 15:45:00|2023|    5|  20|  15|    45|\n|  3|InvalidDate|31/02/2023|02/31/2023|   InvalidTimestamp|           null|           null|           null|               null|null| null|null|null|  null|\n|  4|       null|      null|      null|               null|           null|           null|           null|               null|null| null|null|null|  null|\n+---+-----------+----------+----------+-------------------+---------------+---------------+---------------+-------------------+----+-----+----+----+------+\n\n"}], "source": "df = df\\\n.withColumn('year',year(df.parsed_timestamp))\\\n.withColumn('month',month(df.parsed_timestamp))\\\n.withColumn('day',dayofmonth(df.parsed_timestamp))\\\n.withColumn('hour',hour(df.parsed_timestamp))\\\n.withColumn('minute',minute(df.parsed_timestamp))\ndf.show()"}, {"cell_type": "code", "execution_count": null, "id": "6abe8f16-8b48-48b5-abbd-00357f4493a3", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 29, "id": "41998032-c7e9-44ae-96fb-f33e7b3601fa", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------------+---------------+--------------+\n|parsed_date_mdy|parsed_date_iso|days_dfference|\n+---------------+---------------+--------------+\n|2023-01-15     |2023-01-15     |0             |\n|2023-05-20     |2023-05-20     |0             |\n|null           |null           |null          |\n|null           |null           |null          |\n+---------------+---------------+--------------+\n\n"}], "source": "from pyspark.sql.functions import datediff\n\ndf = df.withColumn('days_dfference' , datediff(df.parsed_date_mdy,df.parsed_date_iso))\ndf.select('parsed_date_mdy','parsed_date_iso','days_dfference').show(truncate=False)"}, {"cell_type": "code", "execution_count": 31, "id": "7ae96263-dc02-45a9-ba44-55931795435d", "metadata": {}, "outputs": [], "source": "spark.stop()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}