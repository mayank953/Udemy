{"cells": [{"cell_type": "code", "execution_count": null, "id": "1d46ef8a-5210-43da-a0c0-55c54c832715", "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession"}, {"cell_type": "code", "execution_count": 1, "id": "7eafeeb3-ae0e-452a-8a42-9ac53940cbab", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/02/03 13:11:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "spark = SparkSession.builder \\\n.appName(\"Schema Enforcement in Spark\") \\\n.getOrCreate()"}, {"cell_type": "code", "execution_count": null, "id": "a560f20d-f24f-4c78-b2c8-42f6c8eeac72", "metadata": {}, "outputs": [], "source": "df = spark.read \\\n.format('csv')\\\n.option('header',\"true\")\\\n.option('inferSchema','true')\\\n.load('/data/customers_500mb.csv')\n\n#  ---> will be scanning the whole and can also lead to wrong inference , not consistency"}, {"cell_type": "code", "execution_count": null, "id": "b6cf994c-6b01-45cf-9d6b-bd76f5d8f3f3", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "99b7a528-8da5-4594-9d52-8a76969393dc", "metadata": {}, "source": "# Struct Type"}, {"cell_type": "code", "execution_count": 5, "id": "0f7916ad-203a-44e9-9f3d-cad6b453b9c3", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "customer_id,name,city,state,country,registration_date,is_active\n0,Customer_0,Pune,Maharashtra,India,2023-06-29,False\n1,Customer_1,Bangalore,Tamil Nadu,India,2023-12-07,True\n2,Customer_2,Hyderabad,Gujarat,India,2023-10-27,True\n3,Customer_3,Bangalore,Karnataka,India,2023-10-17,False\n4,Customer_4,Ahmedabad,Karnataka,India,2023-03-14,False\n5,Customer_5,Hyderabad,Karnataka,India,2023-07-28,False\n6,Customer_6,Pune,Delhi,India,2023-08-29,False\n7,Customer_7,Ahmedabad,West Bengal,India,2023-12-28,True\n8,Customer_8,Pune,Karnataka,India,2023-06-22,True\n9,Customer_9,Mumbai,Telangana,India,2023-01-05,True\n10,Customer_10,Pune,Gujarat,India,2023-08-05,True\n11,Customer_11,Delhi,West Bengal,India,2023-08-02,False\n12,Customer_12,Chennai,Gujarat,India,2023-11-21,False\n13,Customer_13,Chennai,Karnataka,India,2023-11-06,True\n14,Customer_14,Hyderabad,Tamil Nadu,India,2023-02-07,False\n15,Customer_15,Mumbai,Gujarat,India,2023-03-02,True\n16,Customer_16,Chennai,Karnataka,India,2023-04-05,False\n17,Customer_17,Hyderabad,West Bengal,India"}], "source": "!hadoop fs -head /data/customers_100.csv"}, {"cell_type": "code", "execution_count": 4, "id": "e55f4116-8dd3-4712-a927-8e2b5885eda7", "metadata": {}, "outputs": [], "source": "from pyspark.sql.types import StructType,StructField, IntegerType, FloatType, BooleanType, StringType"}, {"cell_type": "code", "execution_count": 15, "id": "79ec5137-9d7f-421c-9db7-0eb71a98081d", "metadata": {}, "outputs": [], "source": "schema = StructType([\n    StructField('customer_id',IntegerType(),True),\n    StructField('name',StringType(),True),\n    StructField('city',StringType(),True),\n    StructField('state',StringType(),True),\n    StructField('country',StringType(),True),\n    StructField('registration_date',StringType(),True),\n    StructField('is_active',BooleanType(),True),\n])"}, {"cell_type": "code", "execution_count": 13, "id": "bf86db01-c085-46e5-bce2-653b59e1225a", "metadata": {}, "outputs": [], "source": "df = spark.read \\\n.format('csv')\\\n.option('header',\"true\")\\\n.schema(schema)\\\n.load('/data/customers_100.csv')"}, {"cell_type": "code", "execution_count": 10, "id": "93dc50eb-60a3-4c3e-ab1d-4fb412e999c6", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+-----------+---------+-----------+-------+-----------------+---------+\n|customer_id|       name|     city|      state|country|registration_date|is_active|\n+-----------+-----------+---------+-----------+-------+-----------------+---------+\n|          0| Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    false|\n|          1| Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     true|\n|          2| Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     true|\n|          3| Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    false|\n|          4| Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    false|\n|          5| Customer_5|Hyderabad|  Karnataka|  India|       2023-07-28|    false|\n|          6| Customer_6|     Pune|      Delhi|  India|       2023-08-29|    false|\n|          7| Customer_7|Ahmedabad|West Bengal|  India|       2023-12-28|     true|\n|          8| Customer_8|     Pune|  Karnataka|  India|       2023-06-22|     true|\n|          9| Customer_9|   Mumbai|  Telangana|  India|       2023-01-05|     true|\n|         10|Customer_10|     Pune|    Gujarat|  India|       2023-08-05|     true|\n|         11|Customer_11|    Delhi|West Bengal|  India|       2023-08-02|    false|\n|         12|Customer_12|  Chennai|    Gujarat|  India|       2023-11-21|    false|\n|         13|Customer_13|  Chennai|  Karnataka|  India|       2023-11-06|     true|\n|         14|Customer_14|Hyderabad| Tamil Nadu|  India|       2023-02-07|    false|\n|         15|Customer_15|   Mumbai|    Gujarat|  India|       2023-03-02|     true|\n|         16|Customer_16|  Chennai|  Karnataka|  India|       2023-04-05|    false|\n|         17|Customer_17|Hyderabad|West Bengal|  India|       2023-08-21|    false|\n|         18|Customer_18|     Pune|      Delhi|  India|       2023-10-04|     true|\n|         19|Customer_19|  Kolkata|    Gujarat|  India|       2023-02-05|     true|\n+-----------+-----------+---------+-----------+-------+-----------------+---------+\nonly showing top 20 rows\n\n"}], "source": "df.show()"}, {"cell_type": "code", "execution_count": 11, "id": "d64ac9e2-98c7-41f9-8a39-165c849e67d7", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- customer_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- country: string (nullable = true)\n |-- registration_date: string (nullable = true)\n |-- is_active: boolean (nullable = true)\n\n"}], "source": "df.printSchema()"}, {"cell_type": "code", "execution_count": null, "id": "0ebfd598-a4cf-45d5-926e-cb0b16869154", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "80618480-de78-4360-a858-9c3e3853340e", "metadata": {}, "source": "# DDL Schema"}, {"cell_type": "code", "execution_count": 26, "id": "7770d82b-ece0-4284-9b2c-53c53b0b77b3", "metadata": {}, "outputs": [], "source": "ddl_schema = 'customer_id INT, name STRING, city STRING,state STRING, country STRING, registration_date STRING, is_active BOOLEAN'"}, {"cell_type": "code", "execution_count": 27, "id": "e99bc9d7-5ce7-47fc-8f87-456af8368ed6", "metadata": {}, "outputs": [], "source": "df_ddl = spark.read \\\n.format('csv')\\\n.option('header',\"true\")\\\n.schema(ddl_schema)\\\n.load('/data/customers_100.csv')"}, {"cell_type": "code", "execution_count": 28, "id": "554b441b-4295-4770-93a4-ec9c5d929a1a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- customer_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- country: string (nullable = true)\n |-- registration_date: string (nullable = true)\n\n"}], "source": "df_ddl.printSchema()"}, {"cell_type": "code", "execution_count": 29, "id": "c562c3d8-89f4-4c90-8fdc-ab435baa21d5", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+-----------+---------+-----------+-------+-----------------+\n|customer_id|       name|     city|      state|country|registration_date|\n+-----------+-----------+---------+-----------+-------+-----------------+\n|          0| Customer_0|     Pune|Maharashtra|  India|       2023-06-29|\n|          1| Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|\n|          2| Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|\n|          3| Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|\n|          4| Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|\n|          5| Customer_5|Hyderabad|  Karnataka|  India|       2023-07-28|\n|          6| Customer_6|     Pune|      Delhi|  India|       2023-08-29|\n|          7| Customer_7|Ahmedabad|West Bengal|  India|       2023-12-28|\n|          8| Customer_8|     Pune|  Karnataka|  India|       2023-06-22|\n|          9| Customer_9|   Mumbai|  Telangana|  India|       2023-01-05|\n|         10|Customer_10|     Pune|    Gujarat|  India|       2023-08-05|\n|         11|Customer_11|    Delhi|West Bengal|  India|       2023-08-02|\n|         12|Customer_12|  Chennai|    Gujarat|  India|       2023-11-21|\n|         13|Customer_13|  Chennai|  Karnataka|  India|       2023-11-06|\n|         14|Customer_14|Hyderabad| Tamil Nadu|  India|       2023-02-07|\n|         15|Customer_15|   Mumbai|    Gujarat|  India|       2023-03-02|\n|         16|Customer_16|  Chennai|  Karnataka|  India|       2023-04-05|\n|         17|Customer_17|Hyderabad|West Bengal|  India|       2023-08-21|\n|         18|Customer_18|     Pune|      Delhi|  India|       2023-10-04|\n|         19|Customer_19|  Kolkata|    Gujarat|  India|       2023-02-05|\n+-----------+-----------+---------+-----------+-------+-----------------+\nonly showing top 20 rows\n\n"}], "source": "df_ddl.show()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}