{"cells": [{"cell_type": "code", "execution_count": 19, "id": "1eeab383-62f6-415f-8e4f-78e149d8ceda", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/02/05 11:30:05 INFO SparkEnv: Registering MapOutputTracker\n25/02/05 11:30:05 INFO SparkEnv: Registering BlockManagerMaster\n25/02/05 11:30:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n25/02/05 11:30:06 INFO SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "from pyspark.sql import SparkSession\n\n\nspark = SparkSession.builder \\\n.appName('Spark SQL Managed vs External') \\\n.enableHiveSupport() \\\n.getOrCreate()"}, {"cell_type": "code", "execution_count": 20, "id": "cd06676c-6b6a-45af-b3aa-05e9b66f58d5", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df = spark.read\\\n.format('csv')\\\n.option('inferScema',\"True\")\\\n.option('header','True')\\\n.load('/data/customers_100.csv')"}, {"cell_type": "code", "execution_count": null, "id": "35cddbb7-fb3d-48ae-a382-362ca96bb21e", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 21, "id": "06ef2a97-1317-488f-b13d-c46cc6086c06", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------+---------+-----------+-------+-----------------+---------+\n|customer_id|      name|     city|      state|country|registration_date|is_active|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n|          0|Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    False|\n|          1|Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     True|\n|          2|Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     True|\n|          3|Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    False|\n|          4|Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    False|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\nonly showing top 5 rows\n\n"}], "source": "df.show(5)"}, {"cell_type": "code", "execution_count": null, "id": "6471417e-25d0-42de-a45d-770d8e858c5e", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 23, "id": "e5dfa9c7-4849-43c2-adb7-29faa1529890", "metadata": {}, "outputs": [], "source": "df.createOrReplaceTempView('temp_customers')"}, {"cell_type": "code", "execution_count": 24, "id": "556bdf13-52ed-4da4-81f0-0ae67c198232", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------+---------+-----------+-------+-----------------+---------+\n|customer_id|      name|     city|      state|country|registration_date|is_active|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n|          0|Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    False|\n|          1|Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     True|\n|          2|Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     True|\n|          3|Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    False|\n|          4|Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    False|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n\n"}], "source": "spark.sql('select * from temp_customers limit 5').show()"}, {"cell_type": "code", "execution_count": 27, "id": "64063123-c115-44b3-a545-a885dec0fd59", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+--------------+-----------+\n|namespace|     tableName|isTemporary|\n+---------+--------------+-----------+\n|         |temp_customers|       true|\n+---------+--------------+-----------+\n\n"}], "source": "spark.sql('show tables').show()"}, {"cell_type": "code", "execution_count": 26, "id": "7ab246b2-195e-4e7a-97c7-73efc052a901", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "++\n||\n++\n++\n\n"}], "source": "spark.sql('drop table customers').show()"}, {"cell_type": "code", "execution_count": 30, "id": "5f1849f0-6df2-4c37-97f5-b1cf2ee64a14", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/02/05 11:34:42 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n25/02/05 11:34:42 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n                                                                                \r"}, {"data": {"text/plain": "DataFrame[]"}, "execution_count": 30, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql('create table managed_customers as select * from temp_customers')"}, {"cell_type": "code", "execution_count": 31, "id": "3e2dbb71-3cb6-4245-8e46-691bd669210f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+---------------------------------------------------------+-------+\n|col_name                    |data_type                                                |comment|\n+----------------------------+---------------------------------------------------------+-------+\n|customer_id                 |string                                                   |null   |\n|name                        |string                                                   |null   |\n|city                        |string                                                   |null   |\n|state                       |string                                                   |null   |\n|country                     |string                                                   |null   |\n|registration_date           |string                                                   |null   |\n|is_active                   |string                                                   |null   |\n|                            |                                                         |       |\n|# Detailed Table Information|                                                         |       |\n|Database                    |default                                                  |       |\n|Table                       |managed_customers                                        |       |\n|Owner                       |root                                                     |       |\n|Created Time                |Wed Feb 05 11:34:42 UTC 2025                             |       |\n|Last Access                 |UNKNOWN                                                  |       |\n|Created By                  |Spark 3.3.2                                              |       |\n|Type                        |MANAGED                                                  |       |\n|Provider                    |hive                                                     |       |\n|Table Properties            |[transient_lastDdlTime=1738755283]                       |       |\n|Statistics                  |5424 bytes                                               |       |\n|Location                    |hdfs://my-cluster-m/user/hive/warehouse/managed_customers|       |\n+----------------------------+---------------------------------------------------------+-------+\nonly showing top 20 rows\n\n"}], "source": "spark.sql('describe extended managed_customers').show(truncate=False)"}, {"cell_type": "code", "execution_count": 32, "id": "90cbc8b0-9fe8-4d97-8f7b-a7b74bcfee0b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 2 items\ndrwxr-xr-x   - root hadoop          0 2025-02-05 10:40 /user/hive/warehouse/ecommerce.db\ndrwxr-xr-x   - root hadoop          0 2025-02-05 11:34 /user/hive/warehouse/managed_customers\n"}], "source": "!hadoop fs -ls /user/hive/warehouse/"}, {"cell_type": "code", "execution_count": null, "id": "9d2d3e27-2691-446f-93d8-6eb3d04b2a3d", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "eaf6eb5d-3900-4feb-8707-fdaeef71d481", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "a9fd6e43-e3ef-4db0-9d2f-cfa1091027e4", "metadata": {}, "source": "# External Table"}, {"cell_type": "code", "execution_count": 45, "id": "bbfced47-01f8-4ca5-b4ae-6eba33c5acd6", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 13 items\n-rw-r--r--   2 root hadoop  343317147 2025-02-03 11:53 /data/customers.csv\n-rw-r--r--   2 root hadoop       5488 2025-02-03 11:53 /data/customers_100.csv\n-rw-r--r--   2 root hadoop   10528211 2025-02-03 11:53 /data/customers_10mb.csv\n-rw-r--r--   2 root hadoop    1060750 2025-02-03 11:53 /data/customers_1mb.csv\n-rw-r--r--   2 root hadoop  570783961 2025-02-03 11:53 /data/customers_500mb.csv\n-rw-r--r--   2 root hadoop        351 2025-02-04 05:35 /data/customers_with_errors.csv\n-rw-r--r--   2 root hadoop         23 2025-02-04 14:53 /data/date.csv\n-rw-r--r--   2 root hadoop        209 2025-02-05 06:28 /data/dates_data.csv\ndrwxr-xr-x   - root hadoop          0 2025-02-05 11:43 /data/external_data\ndrwxr-xr-x   - root hadoop          0 2025-02-04 07:53 /data/write_output.csv\ndrwxr-xr-x   - root hadoop          0 2025-02-04 07:56 /data/write_output_1_part\ndrwxr-xr-x   - root hadoop          0 2025-02-04 07:59 /data/write_output_1_part_2\ndrwxr-xr-x   - root hadoop          0 2025-02-04 08:02 /data/write_output_1_part_3\n"}], "source": "!hadoop fs -ls /data/"}, {"cell_type": "code", "execution_count": 38, "id": "1bf441c4-636a-4307-85dd-bed7f9b85c75", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "customer_id,name,city,state,country,registration_date,is_active\n0,Customer_0,Pune,Maharashtra,India,2023-06-29,False\n1,Customer_1,Bangalore,Tamil Nadu,India,2023-12-07,True\n2,Customer_2,Hyderabad,Gujarat,India,2023-10-27,True\n3,Customer_3,Bangalore,Karnataka,India,2023-10-17,False\n4,Customer_4,Ahmedabad,Karnataka,India,2023-03-14,False\n5,Customer_5,Hyderabad,Karnataka,India,2023-07-28,False\n6,Customer_6,Pune,Delhi,India,2023-08-29,False\n7,Customer_7,Ahmedabad,West Bengal,India,2023-12-28,True\n8,Customer_8,Pune,Karnataka,India,2023-06-22,True\n9,Customer_9,Mumbai,Telangana,India,2023-01-05,True\n10,Customer_10,Pune,Gujarat,India,2023-08-05,True\n11,Customer_11,Delhi,West Bengal,India,2023-08-02,False\n12,Customer_12,Chennai,Gujarat,India,2023-11-21,False\n13,Customer_13,Chennai,Karnataka,India,2023-11-06,True\n14,Customer_14,Hyderabad,Tamil Nadu,India,2023-02-07,False\n15,Customer_15,Mumbai,Gujarat,India,2023-03-02,True\n16,Customer_16,Chennai,Karnataka,India,2023-04-05,False\n17,Customer_17,Hyderabad,West Bengal,India"}], "source": "!hadoop fs -head /data/external_data/customers_1mb.csv"}, {"cell_type": "code", "execution_count": 57, "id": "bc211c29-98cb-4d86-8c64-5a99f81509f1", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/02/05 11:55:14 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `default`.`external_customers_2` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"}, {"data": {"text/plain": "DataFrame[]"}, "execution_count": 57, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql('''\ncreate external table external_customers_2 (\ncustomer_id INT,\nname STRING,\ncity STRING,\nstate string,\ncountry string,\nregistration_date STRING,\nis_active BOOLEAN\n)\nusing csv\nlocation '/data/external_data/'\n''')\n"}, {"cell_type": "code", "execution_count": 41, "id": "4759da98-a740-4919-aebb-7293b92388fc", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+--------------------------------------------------+-------+\n|col_name                    |data_type                                         |comment|\n+----------------------------+--------------------------------------------------+-------+\n|customer_id                 |int                                               |null   |\n|name                        |string                                            |null   |\n|city                        |string                                            |null   |\n|state                       |string                                            |null   |\n|country                     |string                                            |null   |\n|registration_date           |string                                            |null   |\n|is_active                   |boolean                                           |null   |\n|                            |                                                  |       |\n|# Detailed Table Information|                                                  |       |\n|Database                    |default                                           |       |\n|Table                       |external_customers_2                              |       |\n|Owner                       |root                                              |       |\n|Created Time                |Wed Feb 05 11:44:52 UTC 2025                      |       |\n|Last Access                 |UNKNOWN                                           |       |\n|Created By                  |Spark 3.3.2                                       |       |\n|Type                        |EXTERNAL                                          |       |\n|Provider                    |csv                                               |       |\n|Location                    |hdfs://my-cluster-m/data/external_data            |       |\n|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe|       |\n|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat  |       |\n+----------------------------+--------------------------------------------------+-------+\nonly showing top 20 rows\n\n"}], "source": "spark.sql('describe extended external_customers_2').show(truncate=False)"}, {"cell_type": "code", "execution_count": 44, "id": "f5300078-a959-42d2-9b8b-9f3bc0e9f7f8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------+---------+-----------+-------+-----------------+---------+\n|customer_id|      name|     city|      state|country|registration_date|is_active|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n|       null|      name|     city|      state|country|registration_date|     null|\n|          0|Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    false|\n|          1|Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     true|\n|          2|Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     true|\n|          3|Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    false|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n\n"}], "source": "spark.sql('select * from external_customers_2 limit 5').show()"}, {"cell_type": "code", "execution_count": 46, "id": "7ab88feb-1400-4dac-a8f1-3574184a3567", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 2 items\ndrwxr-xr-x   - root hadoop          0 2025-02-05 10:40 /user/hive/warehouse/ecommerce.db\ndrwxr-xr-x   - root hadoop          0 2025-02-05 11:34 /user/hive/warehouse/managed_customers\n"}], "source": "!hadoop fs -ls /user/hive/warehouse/"}, {"cell_type": "code", "execution_count": 47, "id": "a6d48399-eff4-44b0-a80e-3c7d51389909", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+--------------------+-----------+\n|namespace|           tableName|isTemporary|\n+---------+--------------------+-----------+\n|  default|  external_customers|      false|\n|  default|external_customers_2|      false|\n|  default|   managed_customers|      false|\n|         |      temp_customers|       true|\n+---------+--------------------+-----------+\n\n"}], "source": "spark.sql('show tables').show()"}, {"cell_type": "code", "execution_count": 48, "id": "540019b8-0614-47e8-bad1-0a9400dc0d4d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "++\n||\n++\n++\n\n"}], "source": "spark.sql('drop table managed_customers').show()"}, {"cell_type": "code", "execution_count": 49, "id": "655e68be-f0ce-45fc-86eb-49a44783fd41", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 1 items\ndrwxr-xr-x   - root hadoop          0 2025-02-05 10:40 /user/hive/warehouse/ecommerce.db\n"}], "source": "!hadoop fs -ls /user/hive/warehouse/"}, {"cell_type": "code", "execution_count": 50, "id": "9b5d5976-d4c1-4513-bfdd-4a35a7771527", "metadata": {}, "outputs": [{"ename": "AnalysisException", "evalue": "Table or view not found: managed_customers; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [managed_customers], [], false\n", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_35816/514105920.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'select * from managed_customers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: managed_customers; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [managed_customers], [], false\n"]}], "source": "spark.sql('select * from managed_customers').show()"}, {"cell_type": "code", "execution_count": null, "id": "3a1be844-64ae-42d6-9490-5e4fd5d5a451", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 51, "id": "56b6f238-5c4a-4773-b697-9f451ae66c35", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 1 items\n-rw-r--r--   2 root hadoop    1060750 2025-02-05 11:43 /data/external_data/customers_1mb.csv\n"}], "source": "!hadoop fs -ls /data/external_data    "}, {"cell_type": "code", "execution_count": 54, "id": "569ad8c7-647d-4b3f-8c3a-e8f191fc6790", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "++\n||\n++\n++\n\n"}], "source": "spark.sql('drop table external_customers_2').show()"}, {"cell_type": "code", "execution_count": 53, "id": "626b83a7-3d35-4f32-961f-be85a9f3683b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 1 items\n-rw-r--r--   2 root hadoop    1060750 2025-02-05 11:43 /data/external_data/customers_1mb.csv\n"}], "source": "!hadoop fs -ls /data/external_data    "}, {"cell_type": "code", "execution_count": 55, "id": "b96f1906-817e-411e-bf37-94f9ed352ba3", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 13 items\n-rw-r--r--   2 root hadoop  343317147 2025-02-03 11:53 /data/customers.csv\n-rw-r--r--   2 root hadoop       5488 2025-02-03 11:53 /data/customers_100.csv\n-rw-r--r--   2 root hadoop   10528211 2025-02-03 11:53 /data/customers_10mb.csv\n-rw-r--r--   2 root hadoop    1060750 2025-02-03 11:53 /data/customers_1mb.csv\n-rw-r--r--   2 root hadoop  570783961 2025-02-03 11:53 /data/customers_500mb.csv\n-rw-r--r--   2 root hadoop        351 2025-02-04 05:35 /data/customers_with_errors.csv\n-rw-r--r--   2 root hadoop         23 2025-02-04 14:53 /data/date.csv\n-rw-r--r--   2 root hadoop        209 2025-02-05 06:28 /data/dates_data.csv\ndrwxr-xr-x   - root hadoop          0 2025-02-05 11:43 /data/external_data\ndrwxr-xr-x   - root hadoop          0 2025-02-04 07:53 /data/write_output.csv\ndrwxr-xr-x   - root hadoop          0 2025-02-04 07:56 /data/write_output_1_part\ndrwxr-xr-x   - root hadoop          0 2025-02-04 07:59 /data/write_output_1_part_2\ndrwxr-xr-x   - root hadoop          0 2025-02-04 08:02 /data/write_output_1_part_3\n"}], "source": "!hadoop fs -ls /data"}, {"cell_type": "code", "execution_count": null, "id": "429235ad-28ea-45ff-9ed7-d22c2059d245", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 56, "id": "cf3e6e42-f148-4df6-bbff-7fa1174c3412", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+--------------+-----------+\n|namespace|     tableName|isTemporary|\n+---------+--------------+-----------+\n|         |temp_customers|       true|\n+---------+--------------+-----------+\n\n"}], "source": "spark.sql('show tables').show()"}, {"cell_type": "code", "execution_count": null, "id": "a80ecbe4-a3f2-4760-a730-5040946d8f90", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 58, "id": "9e2be13f-c31e-4aa4-b5d2-bd80836ba33e", "metadata": {}, "outputs": [], "source": "spark.stop()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}