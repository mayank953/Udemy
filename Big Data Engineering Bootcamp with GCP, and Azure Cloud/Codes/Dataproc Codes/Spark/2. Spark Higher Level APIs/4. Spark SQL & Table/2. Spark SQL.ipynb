{"cells": [{"cell_type": "code", "execution_count": 1, "id": "408fd633-7796-4cdc-bc77-62601dc36cdf", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/02/05 10:13:48 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "from pyspark.sql import SparkSession\n\n\nspark = SparkSession.builder \\\n.appName('Spark SQL') \\\n.enableHiveSupport() \\\n.getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "id": "2d6a6c51-9946-4108-a648-b793133206e0", "metadata": {}, "outputs": [], "source": "# Insert data into the table from a DataFrame\ndata = [\n(1, \"Alice\", \"Mumbai\", \"2023-01-15\", True),\n(2, \"Bob\", \"Delhi\", \"2023-03-25\", False),\n(3, \"Charlie\", \"Chennai\", \"2023-05-10\", True)\n]\n\ncolumns = [\"customer_id\", \"name\", \"city\", \"registration_date\", \"is_active\"]\n\ndf= spark.createDataFrame(data,columns)"}, {"cell_type": "code", "execution_count": 4, "id": "422ebbcc-a014-4108-b6e7-d2ccc69c252f", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n"}, {"name": "stdout", "output_type": "stream", "text": "+---------+\n|namespace|\n+---------+\n|  default|\n+---------+\n\n"}], "source": "spark.sql('show databases').show()"}, {"cell_type": "code", "execution_count": 5, "id": "49e06448-d09c-4b54-acca-15d54db72dad", "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[]"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql('use default')"}, {"cell_type": "code", "execution_count": 7, "id": "858a5cba-ef2a-4326-8077-80da1c34313e", "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "tags": []}, "outputs": [{"ename": "AnalysisException", "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Unable to create database path file:/spark-warehouse/ecommerce.db, failed to create database ecommerce)", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_35816/3311724696.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'create database if not exists ecommerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Unable to create database path file:/spark-warehouse/ecommerce.db, failed to create database ecommerce)"]}], "source": "spark.sql('create database if not exists ecommerce')"}, {"cell_type": "code", "execution_count": null, "id": "6f291d2a-c000-4218-863b-1e61def648cb", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 8, "id": "f2d309dc-4e2d-4715-bc02-d5d1de62786c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 0:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+--------------------+-----------+\n|namespace|           tableName|isTemporary|\n+---------+--------------------+-----------+\n|  default|customers_persistent|      false|\n+---------+--------------------+-----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql('show tables').show()"}, {"cell_type": "code", "execution_count": 9, "id": "5de471bd-bbaf-4aec-bc21-f520c8aae095", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+----------+---------+-----------+-------+-------------------+---------+\n|customer_id|      name|     city|      state|country|  registration_date|is_active|\n+-----------+----------+---------+-----------+-------+-------------------+---------+\n|          0|Customer_0|     Pune|Maharashtra|  India|2023-06-29 00:00:00|    false|\n|          1|Customer_1|Bangalore| Tamil Nadu|  India|2023-12-07 00:00:00|     true|\n|          2|Customer_2|Hyderabad|    Gujarat|  India|2023-10-27 00:00:00|     true|\n|          3|Customer_3|Bangalore|  Karnataka|  India|2023-10-17 00:00:00|    false|\n|          4|Customer_4|Ahmedabad|  Karnataka|  India|2023-03-14 00:00:00|    false|\n+-----------+----------+---------+-----------+-------+-------------------+---------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql('select * from customers_persistent limit 5').show()\n"}, {"cell_type": "code", "execution_count": 10, "id": "30317653-ca67-44a3-ab64-3cf5dee9e88b", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/02/05 10:43:29 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider CSV. Persisting data source table `default`.`customers` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n25/02/05 10:43:29 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"}, {"data": {"text/plain": "DataFrame[]"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql('''\nCREATE TABLE IF NOT EXISTS customers (\ncustomer_id INT,\nname STRING,\ncity STRING,\nregistration_date STRING,\nis_active BOOLEAN\n) USING CSV\n''')"}, {"cell_type": "code", "execution_count": 11, "id": "325c7cf1-5a95-49c1-b996-025513e58f71", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+--------------------+-----------+\n|namespace|           tableName|isTemporary|\n+---------+--------------------+-----------+\n|  default|           customers|      false|\n|  default|customers_persistent|      false|\n+---------+--------------------+-----------+\n\n"}], "source": "spark.sql('show tables').show()"}, {"cell_type": "code", "execution_count": 12, "id": "5b590dff-1236-4ac3-bfa9-c5d64d3bd781", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+---------------------------------------------------------+-------+\n|col_name                    |data_type                                                |comment|\n+----------------------------+---------------------------------------------------------+-------+\n|customer_id                 |int                                                      |null   |\n|name                        |string                                                   |null   |\n|city                        |string                                                   |null   |\n|registration_date           |string                                                   |null   |\n|is_active                   |boolean                                                  |null   |\n|                            |                                                         |       |\n|# Detailed Table Information|                                                         |       |\n|Database                    |default                                                  |       |\n|Table                       |customers                                                |       |\n|Owner                       |root                                                     |       |\n|Created Time                |Wed Feb 05 10:43:29 UTC 2025                             |       |\n|Last Access                 |UNKNOWN                                                  |       |\n|Created By                  |Spark 3.3.2                                              |       |\n|Type                        |MANAGED                                                  |       |\n|Provider                    |CSV                                                      |       |\n|Location                    |hdfs://my-cluster-m/user/hive/warehouse/customers        |       |\n|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe       |       |\n|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat         |       |\n|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat|       |\n+----------------------------+---------------------------------------------------------+-------+\n\n"}], "source": "spark.sql('describe extended customers').show(truncate=False)"}, {"cell_type": "code", "execution_count": 14, "id": "bff42ae8-35db-4c15-9d56-1082933adbe5", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df.write.mode('overwrite').saveAsTable('default.customers')"}, {"cell_type": "code", "execution_count": 15, "id": "0baeae77-ddfc-4bfc-9f56-a8d778ca6b72", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+-------+-------+-----------------+---------+\n|customer_id|   name|   city|registration_date|is_active|\n+-----------+-------+-------+-----------------+---------+\n|          1|  Alice| Mumbai|       2023-01-15|     true|\n|          2|    Bob|  Delhi|       2023-03-25|    false|\n|          3|Charlie|Chennai|       2023-05-10|     true|\n+-----------+-------+-------+-----------------+---------+\n\n"}], "source": "spark.sql('select * from customers limit 5').show()"}, {"cell_type": "code", "execution_count": 16, "id": "4ee80f98-d86c-45e4-86d0-546650d95881", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "++\n||\n++\n++\n\n"}], "source": "spark.sql('''\ndrop table if exists customers_persistent\n''').show()"}, {"cell_type": "code", "execution_count": null, "id": "78207a95-7dc9-49d1-b661-5b5777517d2a", "metadata": {}, "outputs": [], "source": "# Table \n\n# Data and the Metadata\n\n# Data is stored on HDFS\n# Metadata is on Hive metastore\n\n# Drop the table deletes the metadata and the data"}, {"cell_type": "code", "execution_count": 17, "id": "db13e3fe-d312-4edd-8326-98bd2ea9b696", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+--------------------------------------------------------------+-------+\n|col_name                    |data_type                                                     |comment|\n+----------------------------+--------------------------------------------------------------+-------+\n|customer_id                 |bigint                                                        |null   |\n|name                        |string                                                        |null   |\n|city                        |string                                                        |null   |\n|registration_date           |string                                                        |null   |\n|is_active                   |boolean                                                       |null   |\n|                            |                                                              |       |\n|# Detailed Table Information|                                                              |       |\n|Database                    |default                                                       |       |\n|Table                       |customers                                                     |       |\n|Owner                       |root                                                          |       |\n|Created Time                |Wed Feb 05 10:45:57 UTC 2025                                  |       |\n|Last Access                 |UNKNOWN                                                       |       |\n|Created By                  |Spark 3.3.2                                                   |       |\n|Type                        |MANAGED                                                       |       |\n|Provider                    |parquet                                                       |       |\n|Statistics                  |3034 bytes                                                    |       |\n|Location                    |hdfs://my-cluster-m/user/hive/warehouse/customers             |       |\n|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |       |\n|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |       |\n|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|       |\n+----------------------------+--------------------------------------------------------------+-------+\n\n"}], "source": "spark.sql('''\ndescribe extended customers\n''').show(truncate=False)"}, {"cell_type": "code", "execution_count": 18, "id": "02bd576d-dded-4c44-8d8f-7f8add4dce8a", "metadata": {}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "6ea5ee4c-ccd7-438d-811e-c86efaaba0da", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}