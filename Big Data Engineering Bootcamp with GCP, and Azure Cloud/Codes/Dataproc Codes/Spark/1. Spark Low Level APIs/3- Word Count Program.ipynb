{"cells": [{"cell_type": "code", "execution_count": 1, "id": "eafef72a-cd7d-4028-a2f4-c4c77ed0fb7c", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "hello World\n"}], "source": "print(\"hello World\")"}, {"cell_type": "code", "execution_count": 4, "id": "7ff21bad-acb6-41f3-a342-f1b1fbbfb34f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "10\n"}], "source": "print(5*2)"}, {"cell_type": "raw", "id": "e6096126-c5c2-4ad1-8de8-867cbd2e7b82", "metadata": {}, "source": ""}, {"cell_type": "code", "execution_count": 1, "id": "513b7f07-b6c2-4a69-963b-f2340e21d5dd", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/01/27 16:07:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n.appName(\"WordCountDemo\") \\\n.master(\"yarn\") \\\n.getOrCreate()"}, {"cell_type": "code", "execution_count": 6, "id": "b0e79ee8-134e-435e-94d2-ac207d07c4be", "metadata": {}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://my-cluster-m.us-central1-c.c.bigdata-project-448115.internal:33903\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7fa03c70e7a0>"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": "spark"}, {"cell_type": "code", "execution_count": 31, "id": "88bd6e59-eb78-497e-933c-0cc8f45f7be7", "metadata": {}, "outputs": [], "source": "hdfs_path = '/tmp/inputhdfsdbz.txt'\nrdd = spark.sparkContext.textFile(hdfs_path)"}, {"cell_type": "code", "execution_count": 11, "id": "2b287693-91e3-42cd-b6a8-2fc2c096b929", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "['Goku Vegeta Gohan',\n 'Goku Frieza Goku',\n 'Vegeta Goku Frieza Gohan',\n 'Gohan Frieza Goku Goku']"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "rdd.collect()"}, {"cell_type": "code", "execution_count": null, "id": "c1f78246-8bcb-4ac4-b5cc-92f5f650ee6f", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 12, "id": "9aea81f9-b5bf-4d6b-94a6-f710a0fe7a20", "metadata": {}, "outputs": [], "source": "words = rdd.flatMap(lambda line:line.split(' '))"}, {"cell_type": "code", "execution_count": 13, "id": "1271c90a-cb0d-4170-9b44-cf6733aca59a", "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "['Goku',\n 'Vegeta',\n 'Gohan',\n 'Goku',\n 'Frieza',\n 'Goku',\n 'Vegeta',\n 'Goku',\n 'Frieza',\n 'Gohan',\n 'Gohan',\n 'Frieza',\n 'Goku',\n 'Goku']"}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": "words.collect()"}, {"cell_type": "code", "execution_count": 14, "id": "a2e45cb1-fe20-4189-bb55-ad3fe92c06fd", "metadata": {}, "outputs": [], "source": "word_map = words.map(lambda word:(word,1))"}, {"cell_type": "code", "execution_count": 15, "id": "12ce5296-8900-4a13-b288-d489947160b0", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "[('Goku', 1),\n ('Vegeta', 1),\n ('Gohan', 1),\n ('Goku', 1),\n ('Frieza', 1),\n ('Goku', 1),\n ('Vegeta', 1),\n ('Goku', 1),\n ('Frieza', 1),\n ('Gohan', 1),\n ('Gohan', 1),\n ('Frieza', 1),\n ('Goku', 1),\n ('Goku', 1)]"}, "execution_count": 15, "metadata": {}, "output_type": "execute_result"}], "source": "word_map.collect() "}, {"cell_type": "code", "execution_count": 17, "id": "9f884af0-d73e-4f5a-b05e-00250d399f34", "metadata": {}, "outputs": [], "source": "result = word_map.reduceByKey(lambda a,b : a+b)"}, {"cell_type": "code", "execution_count": 18, "id": "436fcff8-1465-4cac-8665-e906103e4add", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "[('Goku', 6), ('Vegeta', 2), ('Gohan', 3), ('Frieza', 3)]"}, "execution_count": 18, "metadata": {}, "output_type": "execute_result"}], "source": "result.collect()"}, {"cell_type": "code", "execution_count": 32, "id": "e89183e6-e30c-4079-be1a-db81f73278ef", "metadata": {}, "outputs": [], "source": "ans_combined = rdd.flatMap(lambda line:line.split(' ')).map(lambda word:(word,1)).reduceByKey(lambda a,b : a+b)"}, {"cell_type": "code", "execution_count": 20, "id": "c3e86160-55c5-4ab3-8d1c-a0d9cd66170e", "metadata": {}, "outputs": [{"data": {"text/plain": "[('Goku', 6), ('Vegeta', 2), ('Gohan', 3), ('Frieza', 3)]"}, "execution_count": 20, "metadata": {}, "output_type": "execute_result"}], "source": "ans_combined.collect()"}, {"cell_type": "code", "execution_count": null, "id": "7ea9fd32-6117-4a2c-bf9d-386420207204", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 22, "id": "e971abef-c2b2-4c54-ba6f-70b6b551383e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 8 items\n-rw-r--r--   2 root       hadoop    327.4 M 2025-01-20 10:42 /tmp/customers.csv\n-rw-r--r--   2 root       hadoop     10.0 M 2025-01-20 10:31 /tmp/customers_10mb.csv\n-rw-r--r--   2 mayank0953 hadoop      1.0 M 2025-01-20 10:23 /tmp/customers_1mb.csv\n-rw-r--r--   2 mayank0953 hadoop    544.3 M 2025-01-22 17:12 /tmp/customers_500mb.csv\ndrwxr-xr-x   - anonymous  hadoop          0 2025-01-23 16:41 /tmp/external\ndrwxrwxrwt   - hdfs       hadoop          0 2025-01-20 09:57 /tmp/hadoop-yarn\ndrwx-wx-wx   - hive       hadoop          0 2025-01-20 09:57 /tmp/hive\n-rw-r--r--   2 root       hadoop         83 2025-01-26 19:11 /tmp/inputhdfsdbz.txt\n"}], "source": "!hadoop fs -ls -h /tmp/"}, {"cell_type": "code", "execution_count": 6, "id": "d4af1619-2648-4b46-8431-b6c7f966a08e", "metadata": {}, "outputs": [], "source": "hdfs_path = '/tmp/inputhdfsdbz.txt'\nrdd = spark.sparkContext.textFile(hdfs_path)"}, {"cell_type": "code", "execution_count": 7, "id": "56adbf8d-9d03-4db4-a05a-ac0fc3dc90b5", "metadata": {}, "outputs": [], "source": "ans_combined = rdd.flatMap(lambda line:line.split(' ')).map(lambda word:(word,1)).reduceByKey(lambda a,b : a+b)"}, {"cell_type": "code", "execution_count": 8, "id": "f578427e-4a80-4d85-95c6-48b8f13db8e0", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "4"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "ans_combined.count()"}, {"cell_type": "code", "execution_count": 9, "id": "b865708e-045f-4c66-8258-b113b8dcfabf", "metadata": {}, "outputs": [], "source": "hdfs_path = '/tmp/customers_500mb.csv'\nrdd_big_file = spark.sparkContext.textFile(hdfs_path)\nbig_ans_combined = rdd_big_file.flatMap(lambda line:line.split(' ')).map(lambda word:(word,1)).reduceByKey(lambda a,b : a+b)"}, {"cell_type": "code", "execution_count": 10, "id": "d4cd35d7-8606-4f13-9773-2020cb132d8c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "8768819"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "big_ans_combined.count()"}, {"cell_type": "code", "execution_count": 11, "id": "6bea27e2-ba39-4731-9f65-4790e5f24dad", "metadata": {}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "41e776bc-2e7b-4435-9038-00c7962de7cd", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "b24783a8-e1f1-4537-94d1-88e0c446b78c", "metadata": {}, "source": "# Transformation vs Action\n\n# Why is Spark Lazy"}, {"cell_type": "code", "execution_count": 2, "id": "e28f7ada-21ba-4571-8c29-754bf6e5d672", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/01/29 11:35:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n.appName(\"WordCountDemo\") \\\n.master(\"yarn\") \\\n.getOrCreate()"}, {"cell_type": "code", "execution_count": 9, "id": "dddb201f-8de7-4229-b6aa-daac8f6dd0e9", "metadata": {}, "outputs": [], "source": "hdfs_path = '/tmp/inputhdfsdbz.txt'\nrdd_big_file = spark.sparkContext.textFile(hdfs_path)\n"}, {"cell_type": "code", "execution_count": 10, "id": "377e65be-a3fd-43e3-9a45-e8cb0bb845a9", "metadata": {}, "outputs": [], "source": "big_ans_combined = rdd_big_file.flatMap(lambda line:line.split(' ')).map(lambda word:(word,1)).reduceByKey(lambda a,b : a+b)"}, {"cell_type": "code", "execution_count": 11, "id": "4b2b4294-e13a-482c-b02f-bf6ce57789de", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "[('Goku', 6), ('Vegeta', 2), ('Gohan', 3), ('Frieza', 3)]"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "big_ans_combined.collect()"}, {"cell_type": "code", "execution_count": 12, "id": "6a2f484c-c072-4780-a8c1-8873251fd53d", "metadata": {}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "67fd9073-ce54-48c8-bfb9-86d23a4a1645", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "58c9a20c-fd2c-4cba-94bb-896ac6e037ae", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 13, "id": "dc9cd0d5-12ad-480e-a52d-243eb1082e11", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/01/29 11:57:24 INFO SparkEnv: Registering MapOutputTracker\n25/01/29 11:57:24 INFO SparkEnv: Registering BlockManagerMaster\n25/01/29 11:57:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n25/01/29 11:57:24 INFO SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n.appName(\"RDD Intro\") \\\n.master(\"yarn\") \\\n.getOrCreate()"}, {"cell_type": "code", "execution_count": null, "id": "218607d1-8fc3-490c-906f-c751d71265ae", "metadata": {}, "outputs": [], "source": "hdfs_path = '/tmp/inputhdfsdbz.txt'\nrdd_big_file = spark.sparkContext.textFile(hdfs_path)\ns"}, {"cell_type": "code", "execution_count": null, "id": "c79c7655-f1ac-4f65-82a8-29265f71d9df", "metadata": {}, "outputs": [], "source": "rdd1 = rdd_big_file.flatMap(lambda line:line.split(' '))\nrdd1 = rdd1.map(lambda word:(word,1))\nrdd1 = rdd1.reduceByKey(lambda a,b : a+b) # In this case we will have to start from Scratch only."}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}