{"cells": [{"cell_type": "code", "execution_count": null, "id": "d0aa027e-f769-443c-a61c-d755118af89d", "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession\n\n\nspark = SparkSession. builder \\\n.appName (\"Spark_Table_Cache\") \\\n.getOrCreate()"}, {"cell_type": "code", "execution_count": null, "id": "99e58862-9f3e-4d2d-af18-e4acc231c43f", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 1, "id": "1ffc1790-24fa-4025-be17-8bdfd92757db", "metadata": {}, "outputs": [], "source": "customers_schema = 'customers_id INT, name STRING, city STRING, state STRING, country STRING, registration_date STRING, is_active BOOLEAN'"}, {"cell_type": "code", "execution_count": 2, "id": "d5fadcd3-fb8e-4903-a8fa-fd37861daf20", "metadata": {}, "outputs": [], "source": "customers_df = spark.read.format('csv').schema(customers_schema).load('/data/customers_500mb.csv')"}, {"cell_type": "code", "execution_count": null, "id": "e6f36e62-f85d-48e2-ac0e-2d98740b78dc", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 3, "id": "f7835208-fd5b-4ae0-97fd-1feb8a0b3c59", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n25/02/06 11:40:31 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `default`.`customers_500mb` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n25/02/06 11:40:31 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"}], "source": "customers_df.write.format('csv').saveAsTable('default.customers_500mb')"}, {"cell_type": "code", "execution_count": 4, "id": "d7deba5a-0118-45ff-ac4b-f532583e8cae", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+---------+-------+\n|         col_name|data_type|comment|\n+-----------------+---------+-------+\n|     customers_id|      int|   null|\n|             name|   string|   null|\n|             city|   string|   null|\n|            state|   string|   null|\n|          country|   string|   null|\n|registration_date|   string|   null|\n|        is_active|  boolean|   null|\n+-----------------+---------+-------+\n\n"}], "source": "spark.sql('describe customers_500mb').show()"}, {"cell_type": "code", "execution_count": 5, "id": "a3d3d1d6-0548-457a-a85e-0d6698d34992", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+-------------------------------------------------------+-------+\n|col_name                    |data_type                                              |comment|\n+----------------------------+-------------------------------------------------------+-------+\n|customers_id                |int                                                    |null   |\n|name                        |string                                                 |null   |\n|city                        |string                                                 |null   |\n|state                       |string                                                 |null   |\n|country                     |string                                                 |null   |\n|registration_date           |string                                                 |null   |\n|is_active                   |boolean                                                |null   |\n|                            |                                                       |       |\n|# Detailed Table Information|                                                       |       |\n|Database                    |default                                                |       |\n|Table                       |customers_500mb                                        |       |\n|Owner                       |root                                                   |       |\n|Created Time                |Thu Feb 06 11:40:32 UTC 2025                           |       |\n|Last Access                 |UNKNOWN                                                |       |\n|Created By                  |Spark 3.3.2                                            |       |\n|Type                        |MANAGED                                                |       |\n|Provider                    |csv                                                    |       |\n|Statistics                  |570783941 bytes                                        |       |\n|Location                    |hdfs://my-cluster-m/user/hive/warehouse/customers_500mb|       |\n|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe     |       |\n+----------------------------+-------------------------------------------------------+-------+\nonly showing top 20 rows\n\n"}], "source": "spark.sql('describe extended customers_500mb').show(truncate =False)"}, {"cell_type": "code", "execution_count": 8, "id": "fa13a366-6dbf-4e7e-965f-804c39081a33", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 6 items\n-rw-r--r--   2 root hadoop          0 2025-02-06 11:40 /user/hive/warehouse/customers_500mb/_SUCCESS\n-rw-r--r--   2 root hadoop    128.0 M 2025-02-06 11:40 /user/hive/warehouse/customers_500mb/part-00000-0873cff5-0c4b-4aa1-8bbc-12b938afa907-c000.csv\n-rw-r--r--   2 root hadoop    128.0 M 2025-02-06 11:40 /user/hive/warehouse/customers_500mb/part-00001-0873cff5-0c4b-4aa1-8bbc-12b938afa907-c000.csv\n-rw-r--r--   2 root hadoop    128.0 M 2025-02-06 11:40 /user/hive/warehouse/customers_500mb/part-00002-0873cff5-0c4b-4aa1-8bbc-12b938afa907-c000.csv\n-rw-r--r--   2 root hadoop    128.0 M 2025-02-06 11:40 /user/hive/warehouse/customers_500mb/part-00003-0873cff5-0c4b-4aa1-8bbc-12b938afa907-c000.csv\n-rw-r--r--   2 root hadoop     32.3 M 2025-02-06 11:40 /user/hive/warehouse/customers_500mb/part-00004-0873cff5-0c4b-4aa1-8bbc-12b938afa907-c000.csv\n"}], "source": "!hadoop fs -ls -h /user/hive/warehouse/customers_500mb"}, {"cell_type": "code", "execution_count": 9, "id": "338ed470-475d-40b2-8c2a-4748be23c8cb", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------+----------+---------+-----------+-------+-----------------+---------+\n|customers_id|      name|     city|      state|country|registration_date|is_active|\n+------------+----------+---------+-----------+-------+-----------------+---------+\n|        null|      name|     city|      state|country|registration_date|     null|\n|           0|Customer_0|   Mumbai|  Telangana|  India|       2023-03-21|     true|\n|           1|Customer_1|  Chennai|West Bengal|  India|       2023-05-27|    false|\n|           2|Customer_2|     Pune|  Karnataka|  India|       2023-10-11|    false|\n|           3|Customer_3|Hyderabad|    Gujarat|  India|       2023-11-11|    false|\n+------------+----------+---------+-----------+-------+-----------------+---------+\n\n"}], "source": "spark.sql('select * from customers_500mb limit 5').show()"}, {"cell_type": "code", "execution_count": 10, "id": "c618c3b4-cf56-411c-9113-b5bbc1685934", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+-------------------------------------------------------+-------+\n|col_name                    |data_type                                              |comment|\n+----------------------------+-------------------------------------------------------+-------+\n|customers_id                |int                                                    |null   |\n|name                        |string                                                 |null   |\n|city                        |string                                                 |null   |\n|state                       |string                                                 |null   |\n|country                     |string                                                 |null   |\n|registration_date           |string                                                 |null   |\n|is_active                   |boolean                                                |null   |\n|                            |                                                       |       |\n|# Detailed Table Information|                                                       |       |\n|Database                    |default                                                |       |\n|Table                       |customers_500mb                                        |       |\n|Owner                       |root                                                   |       |\n|Created Time                |Thu Feb 06 11:40:32 UTC 2025                           |       |\n|Last Access                 |UNKNOWN                                                |       |\n|Created By                  |Spark 3.3.2                                            |       |\n|Type                        |MANAGED                                                |       |\n|Provider                    |csv                                                    |       |\n|Statistics                  |570783941 bytes                                        |       |\n|Location                    |hdfs://my-cluster-m/user/hive/warehouse/customers_500mb|       |\n|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe     |       |\n+----------------------------+-------------------------------------------------------+-------+\nonly showing top 20 rows\n\n"}], "source": "spark.sql('describe extended customers_500mb').show(truncate =False)"}, {"cell_type": "code", "execution_count": null, "id": "235570b4-73c1-4e21-a2c8-a9de661f5683", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 12, "id": "a00eed46-4903-465e-b14e-4e57baac4662", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 4:===============================================>           (4 + 1) / 5]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------+\n|count(8)|\n+--------+\n| 8767359|\n+--------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql('select count(8) from customers_500mb').show()"}, {"cell_type": "code", "execution_count": 13, "id": "1891041a-692e-40cd-a7e4-f9a2e0338349", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "DataFrame[]"}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql('cache table default.customers_500mb') #Eager Caching"}, {"cell_type": "code", "execution_count": 15, "id": "74f5c987-2d5e-4e2a-81e0-0c6bd7d4853e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------+\n|count(1)|\n+--------+\n| 8767359|\n+--------+\n\n"}], "source": "spark.sql('select count(*) from default.customers_500mb').show()"}, {"cell_type": "code", "execution_count": 16, "id": "80f93cd5-9181-49fd-8a7f-01f87c988875", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------+----------+---------+-----------+-------+-----------------+---------+\n|customers_id|      name|     city|      state|country|registration_date|is_active|\n+------------+----------+---------+-----------+-------+-----------------+---------+\n|        null|      name|     city|      state|country|registration_date|     null|\n|           0|Customer_0|   Mumbai|  Telangana|  India|       2023-03-21|     true|\n|           1|Customer_1|  Chennai|West Bengal|  India|       2023-05-27|    false|\n|           2|Customer_2|     Pune|  Karnataka|  India|       2023-10-11|    false|\n|           3|Customer_3|Hyderabad|    Gujarat|  India|       2023-11-11|    false|\n+------------+----------+---------+-----------+-------+-----------------+---------+\n\n"}], "source": "spark.sql('select (*) from default.customers_500mb limit 5').show()"}, {"cell_type": "code", "execution_count": 17, "id": "92e0f30e-129b-49d3-a4a3-1cbf896af4ef", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 16:==================================>                       (3 + 2) / 5]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+--------+\n|     city|count(1)|\n+---------+--------+\n|Bangalore| 1094195|\n|  Chennai| 1095052|\n|   Mumbai| 1095815|\n|Ahmedabad| 1097162|\n|  Kolkata| 1096777|\n|     Pune| 1095748|\n|    Delhi| 1096183|\n|Hyderabad| 1096426|\n|     city|       1|\n+---------+--------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql('select city,count(*) from customers_500mb group by city').show()"}, {"cell_type": "code", "execution_count": 19, "id": "fb122991-3927-4bdd-a41e-3dac3091ee59", "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[]"}, "execution_count": 19, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql('uncache table customers_500mb')"}, {"cell_type": "code", "execution_count": 20, "id": "0a38a61c-dfa5-4917-bd1e-1c267dd9f8ee", "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[]"}, "execution_count": 20, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql('cache lazy table customers_500mb')"}, {"cell_type": "code", "execution_count": 21, "id": "5e22b8b8-9a6e-40ea-b235-94e6e770d8f4", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 19:==============================================>           (4 + 1) / 5]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------+\n|count(1)|\n+--------+\n| 8767359|\n+--------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql('select count(*) from customers_500mb').show()"}, {"cell_type": "code", "execution_count": 22, "id": "988734cc-aea3-4fa8-88ab-92617b888c8a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+--------------------+-----------+\n|namespace|           tableName|isTemporary|\n+---------+--------------------+-----------+\n|  default|     customers_500mb|      false|\n|  default|external_customers_2|      false|\n+---------+--------------------+-----------+\n\n"}], "source": "spark.sql('show tables').show()"}, {"cell_type": "markdown", "id": "a684937a-f414-4437-b812-0ea9e77789a7", "metadata": {}, "source": "# External Table"}, {"cell_type": "code", "execution_count": 23, "id": "6dc1e405-b6b6-4779-a3a6-45da7d54a5ae", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+-----------+---------+-----------+-------+-----------------+---------+\n|customer_id|       name|     city|      state|country|registration_date|is_active|\n+-----------+-----------+---------+-----------+-------+-----------------+---------+\n|       null|       name|     city|      state|country|registration_date|     null|\n|          0| Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    false|\n|          1| Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     true|\n|          2| Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     true|\n|          3| Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    false|\n|          4| Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    false|\n|          5| Customer_5|Hyderabad|  Karnataka|  India|       2023-07-28|    false|\n|          6| Customer_6|     Pune|      Delhi|  India|       2023-08-29|    false|\n|          7| Customer_7|Ahmedabad|West Bengal|  India|       2023-12-28|     true|\n|          8| Customer_8|     Pune|  Karnataka|  India|       2023-06-22|     true|\n|          9| Customer_9|   Mumbai|  Telangana|  India|       2023-01-05|     true|\n|         10|Customer_10|     Pune|    Gujarat|  India|       2023-08-05|     true|\n|         11|Customer_11|    Delhi|West Bengal|  India|       2023-08-02|    false|\n|         12|Customer_12|  Chennai|    Gujarat|  India|       2023-11-21|    false|\n|         13|Customer_13|  Chennai|  Karnataka|  India|       2023-11-06|     true|\n|         14|Customer_14|Hyderabad| Tamil Nadu|  India|       2023-02-07|    false|\n|         15|Customer_15|   Mumbai|    Gujarat|  India|       2023-03-02|     true|\n|         16|Customer_16|  Chennai|  Karnataka|  India|       2023-04-05|    false|\n|         17|Customer_17|Hyderabad|West Bengal|  India|       2023-08-21|    false|\n|         18|Customer_18|     Pune|      Delhi|  India|       2023-10-04|     true|\n+-----------+-----------+---------+-----------+-------+-----------------+---------+\nonly showing top 20 rows\n\n"}], "source": "spark.sql('select * from external_customers_2').show()"}, {"cell_type": "code", "execution_count": 24, "id": "62249f74-da69-4d62-a792-ef48a9589ec5", "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[]"}, "execution_count": 24, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql('cache table external_customers_2')"}, {"cell_type": "code", "execution_count": null, "id": "2fbb5d84-c288-4ad8-a793-8c50f326d07f", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "45435ae4-fce2-4ee3-bc36-d50c5ac77b25", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 25, "id": "0ccba51b-828d-42a8-b8f3-b07db539d76b", "metadata": {}, "outputs": [], "source": "spark.catalog.clearCache()"}, {"cell_type": "code", "execution_count": 26, "id": "b15edefc-2de1-432d-ba6e-554d858b7e25", "metadata": {}, "outputs": [], "source": "spark.stop()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}